{
 "cells": [
  {
   "source": [
    "## Part 2 A.1: Inverted Index Optimization: Index Creation\n",
    "\n",
    "As evaluated in the vector space model, 99.84% of the document matrix was empty and hence, a lot of space was being wasted. This could be prevented by using the inverted index representation. \n",
    "\n",
    "\n",
    "For every term present in the vocabolary, there exists a posting list which contains all the documents and their term frequency as nodes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import string\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys "
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "source": [
    "To view the memory usage by the program, tracemalloc module is used. The values of current and peak memory usage are printed at the end of the program.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracemalloc\n",
    "\n",
    "tracemalloc.start()"
   ]
  },
  {
   "source": [
    "The follow class contains:\n",
    "+ Dictionary from term to posting list\n",
    "+ Dictionary that stores the total magnitude (c-normalization) for every document\n",
    "+ Function to initialize the above mentioned dictionaries\n",
    "+ Class definitions for posting list and its nodes\n",
    "\n",
    "\n",
    "Steps similar to vector space generation are followed, but instead of two passes, only one is required as we process a whole document at once and the following algorithm is used:\n",
    "+ if term does not exist in dictionary, create a posting list for the term and initialize document frequency to 1\n",
    "+ if term exists in the dictionary, and the document is not in posting list, add the node to the posting list initializing the term frequency to 1 and updating the document frequency of the term.\n",
    "+ if term exists in the dictionary, and the document exists in the posting list, update its term frequency.\n",
    "+ While processing a single document, seperately store all the term frequencies in a vector and find its magnitude for c-normalization\n",
    "\n",
    "All the frequencies are stored as integers and not in its logarithmic float variants to conserve space \n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Inverted_Index:\n",
    "    def __init__(self, FILENAME):\n",
    "        self.FILENAME = FILENAME\n",
    "        # dictionary from docID to docTitle\n",
    "        self.docs = {}\n",
    "\n",
    "        # magnitude of tf vector of each document \n",
    "        self.magnitude = dict()\n",
    "        \n",
    "        # term -> posting list dictionary  \n",
    "        self.InvertedIndex = {}\n",
    "\n",
    "        self.initialize()\n",
    "    \n",
    "\n",
    "\n",
    "    class PostingList:\n",
    "        def __init__(self, docID):\n",
    "            self.head = self.Node(docID)\n",
    "            self.tail = self.Node(docID)\n",
    "\n",
    "        # adding a word to the posting list \n",
    "        def add(self, docID):\n",
    "            if self.tail.docID == docID:\n",
    "                if self.head.docID == self.tail.docID:\n",
    "                    self.head.tf += 1\n",
    "                self.tail.tf += 1\n",
    "            else:\n",
    "                temp = self.Node(docID)\n",
    "                if self.head.docID == self.tail.docID:\n",
    "                    self.head.next = temp\n",
    "                self.tail.next = temp\n",
    "                self.tail = temp\n",
    "\n",
    "        # nodes in the posting list \n",
    "        class Node:\n",
    "            def __init__(self, docID):\n",
    "                self.docID = docID\n",
    "                self.tf = 1\n",
    "                self.next = None\n",
    "\n",
    "\n",
    "    def initialize(self):\n",
    "        with open(self.FILENAME) as file:\n",
    "            seen = {}\n",
    "            for line in file:\n",
    "                line = line.lower()\n",
    "                check = re.search('<doc id=\"(.*?)\" url=\"(.*?)\" title=\"(.*?)\">', line)\n",
    "                if check:\n",
    "                    # 0 -> entire string, 1 -> id, 2 -> url, 3 -> title\n",
    "                    seen = {}\n",
    "                    self.docs[int(check[1])] = check[3]\n",
    "                    current_doc = int(check[1])\n",
    "                    continue\n",
    "                    \n",
    "                if line == '\\n' or line == '</doc>\\n':\n",
    "                    continue\n",
    "                \n",
    "                # cleaning the line \n",
    "                line = re.sub('<a.*?>|</a>', '', line)\n",
    "                line = re.sub('-|–|—', ' ', line)\n",
    "                line = line.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "\n",
    "                # for every word encountered in document, add it to posting list \n",
    "                for term in line.strip().split(' '):\n",
    "                    if term in self.InvertedIndex.keys():\n",
    "                        self.InvertedIndex[term][0].add(current_doc)\n",
    "                    else:\n",
    "                        self.InvertedIndex[term] = [self.PostingList(current_doc), 0]\n",
    "\n",
    "                    if not term in seen.keys():\n",
    "                        seen[term] = 1\n",
    "                        self.InvertedIndex[term][1] += 1\n",
    "                    else:\n",
    "                        seen[term] += 1\n",
    "\n",
    "                # magnitude of (1+log(tf)) vector for each document \n",
    "                self.magnitude[current_doc] = np.linalg.norm(np.array(list(map(lambda x : (1+math.log2(x)) ,seen.values()))))\n",
    "\n",
    "        self.no_documents = len(self.docs)"
   ]
  },
  {
   "source": [
    "### Creating and saving the model\n",
    "We create a new model for the wiki_93 dataset. The object created is saved using the pickle module"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new system for the wiki_93 file\n",
    "index = Inverted_Index('./wiki_93')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(10**6)\n",
    "file = open(\"inverted_index_obj.pickle\", \"wb\")\n",
    "pickle.dump(index, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Current memory usage is 126.894685MB; Peak was 227.810828MB\n"
     ]
    }
   ],
   "source": [
    "current, peak = tracemalloc.get_traced_memory()\n",
    "print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "source": [
    "In our testing, this index creation program had a peak memory usage of 247.7MB. Also the final pickle object has the size 26.7 MB Which is a huge improvement from only vector space model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pythonjvsc74a57bd0ddd3d7e86cbbc0e08bca60a5fc2395220a1f045b0d1d96eafb3eb8348c10d91f",
   "display_name": "Python 3.8.5  ('ir_env': venv)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "ddd3d7e86cbbc0e08bca60a5fc2395220a1f045b0d1d96eafb3eb8348c10d91f"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}